{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat, reduce, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image 를 Patch Size로 자른다.\n",
    "class image_embedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, img_size = 224, patch_size = 16,emb_dim = 16*16*3 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rearrange = rearrange(torch.zeros((1,1,1,1)),'b c (num_w p1)(num_h p2) -> b (num_w num_h)(p1 p2 c)', p1 = patch_size, p2 = patch_size)\n",
    "        self.linear = nn.Linear(in_channels+patch_size*patch_size, emb_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "\n",
    "        n_patchs = img_size * img_size // patch_size **2\n",
    "        self.positions = nn.Parameter(torch.randn(n_patchs + 1, emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, channel, width, height  = x.shape\n",
    "        # print(\"Before rearranfe x shape:\", x.shape )\n",
    "        x = self.rearrange(tensor = x)\n",
    "        # print(\"After rearranfe x shape:\", x.shape )\n",
    "        x = self.linear(x)\n",
    "        # print(\"cls_token shape:\", self.cls_token.shape)\n",
    "        c = repeat(self.cls_token, \"() n d -> b n d\", b = batch)\n",
    "        x = torch.cat((c,x), 1)\n",
    "        # print(\"add cls_token shape:\", x.shape)\n",
    "        # print(\"positions shape:\", self.positions.shape)\n",
    "        x = torch.add(x, self.positions)\n",
    "        print('last shape', x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer Encorder Part\n",
    "class mutil_head_attention(nn.Module):\n",
    "    def __init__(self, emb_dim:int = 16*16*3, num_heads:int = 8, dropout_ratio:float = 0.2, verbose = False, **kwargs):\n",
    "        super(mutil_head_attention,self).__init__()\n",
    "        self.v = verbose\n",
    "        self.emb_dim  = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.scaling = (self.emb_dim // self.num_heads) ** (-0.5)\n",
    "\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.att_drop = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.linear = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        if self.v : print(Q.size(), K.size(), V.size())\n",
    "        # q=k=v = path_size * 2 + 1 & h*d = emd_dim\n",
    "        Q = rearrange(Q, 'b q (h d) -> b h q d', h= self.num_heads)\n",
    "        K = rearrange(K, 'b k (h d) -> b h d k', h = self.num_heads)\n",
    "        V = rearrange(V, 'b V (h d) -> b h v d', h = self.num_heads)\n",
    "        if self.v : print(Q.size(), K.size(), V.size())\n",
    "\n",
    "        weight = torch.matmul(Q,K)\n",
    "        weight = weight * self.scaling\n",
    "\n",
    "        if self.v : print(weight.size())\n",
    "\n",
    "        attention = torch.softmax(weight, dim = -1)\n",
    "        attention = self.att_drop(attention)\n",
    "        if self.v : print(weight.size())\n",
    "\n",
    "        contex = torch.matmul(attention,V)\n",
    "        contex = rearrange(contex, 'b h q d -> b q (h d)')\n",
    "        if self.v: print(contex.size())\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP Block \n",
    "class mlp_block(nn.Module):\n",
    "    def __init__(self, emb_dim:int = 16*16*3, forward_dim:int = 4, dropout_ratio:float = 0.2, **kwargs):\n",
    "        super(mlp_block, self).__init__()\n",
    "        self.linear_1 =  nn.Linear(emb_dim, forward_dim * emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.linear_2 = nn.Linear(emb_dim * forward_dim, emb_dim)\n",
    "\n",
    "    def foreard(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder Block\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, emb_dim:int = 16*16*3, num_heads:int = 8, forward_dim:int = 4, dropout_ratio:float = 0.2):\n",
    "        super(encoder_block, self).__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm_1_G = nn.GELU()\n",
    "        self.mha = mutil_head_attention(emb_dim, num_heads, dropout_ratio)\n",
    "\n",
    "        self.norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.norm_2_G = nn.GELU()\n",
    "        self.mlp = mlp_block(emb_dim, forward_dim, dropout_ratio)\n",
    "        self.residual_dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.norm_1(x)\n",
    "        x2 = self.norm_1_G(x)\n",
    "        x2, attention = self.mha(x2)\n",
    "        x = torch.add(x2, x)\n",
    "\n",
    "        x2 = self.norm_2(x)\n",
    "        x2 = self.norm_2_G(x)\n",
    "        x2 = self.mlp(x2)\n",
    "        x = torch.add(x2,x)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vision_transformer(nn.Module):\n",
    "    def __init__(self, in_channel:int = 3, img_size:int = 224, patch_size:int = 16, emb_dim:int = 16*16*3,\n",
    "                 n_enc_layers:int = 15, num_heads:int= 4, forward_dim:int=4, dropout_ratio:float = 0.2, n_classes:int = 1000):\n",
    "        super(vision_transformer, self).__init__()\n",
    "\n",
    "        self.image_embedding = image_embedding(in_channel, img_size, patch_size, emb_dim)\n",
    "        encoder_module = [ encoder_block(emb_dim, num_heads, forward_dim, dropout_ratio) for _ in range(n_enc_layers)]\n",
    "        self.encoder_module = nn.ModuleList(encoder_module)\n",
    "\n",
    "        self.reduce_layer = reduce('b n e -> b e', reduction='mean')\n",
    "        self.nomalization = nn.LayerNorm(emb_dim)\n",
    "        self.classification_head = nn.Linear(emb_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_embedding(x)\n",
    "        attentions = [block(x)[1] for block in self.encoder_module]\n",
    "        # print(\"before reduce x size:\", x)\n",
    "        x = self.reduce_layer(x)\n",
    "        x = self.nomalization(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classification_head(x)(8,1000)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b c (num_w p1)(num_h p2) -> b (num_w num_h)(p1 p2 c)\".\n Input tensor shape: torch.Size([1, 1, 1, 1]). Additional info: {'p1': 16, 'p2': 16}.\n Shape mismatch, can't divide axis of length 1 in chunks of 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\einops\\einops.py:412\u001b[0m, in \u001b[0;36mreduce\u001b[1;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[0;32m    411\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39mhashable_axes_lengths)\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39;49mreduction)\n\u001b[0;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\einops\\einops.py:235\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[1;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[0;32m    233\u001b[0m backend \u001b[39m=\u001b[39m get_backend(tensor)\n\u001b[0;32m    234\u001b[0m init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 235\u001b[0m     _reconstruct_from_shape(recipe, backend\u001b[39m.\u001b[39;49mshape(tensor))\n\u001b[0;32m    236\u001b[0m tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[1;32mc:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\einops\\einops.py:200\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(length, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(known_product, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m length \u001b[39m%\u001b[39m known_product \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m\"\u001b[39m\u001b[39mShape mismatch, can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt divide axis of length \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m in chunks of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    201\u001b[0m         length, known_product))\n\u001b[0;32m    203\u001b[0m unknown_axis \u001b[39m=\u001b[39m unknown_axes[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mEinopsError\u001b[0m: Shape mismatch, can't divide axis of length 1 in chunks of 16",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m vision_transformer()\n\u001b[0;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m, in \u001b[0;36mvision_transformer.__init__\u001b[1;34m(self, in_channel, img_size, patch_size, emb_dim, n_enc_layers, num_heads, forward_dim, dropout_ratio, n_classes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_channel:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, img_size:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m224\u001b[39m, patch_size:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m, emb_dim:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\u001b[39m*\u001b[39m\u001b[39m16\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m      3\u001b[0m              n_enc_layers:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m, num_heads:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, forward_dim:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, dropout_ratio:\u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, n_classes:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[39msuper\u001b[39m(vision_transformer, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_embedding \u001b[39m=\u001b[39m image_embedding(in_channel, img_size, patch_size, emb_dim)\n\u001b[0;32m      7\u001b[0m     encoder_module \u001b[39m=\u001b[39m [ encoder_block(emb_dim, num_heads, forward_dim, dropout_ratio) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_enc_layers)]\n\u001b[0;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_module \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(encoder_module)\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mimage_embedding.__init__\u001b[1;34m(self, in_channels, img_size, patch_size, emb_dim)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_channels \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, img_size \u001b[39m=\u001b[39m \u001b[39m224\u001b[39m, patch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m,emb_dim \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\u001b[39m*\u001b[39m\u001b[39m16\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m ):\n\u001b[0;32m      4\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrearrange \u001b[39m=\u001b[39m rearrange(torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m)),\u001b[39m'\u001b[39;49m\u001b[39mb c (num_w p1)(num_h p2) -> b (num_w num_h)(p1 p2 c)\u001b[39;49m\u001b[39m'\u001b[39;49m, p1 \u001b[39m=\u001b[39;49m patch_size, p2 \u001b[39m=\u001b[39;49m patch_size)\n\u001b[0;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_channels\u001b[39m+\u001b[39mpatch_size\u001b[39m*\u001b[39mpatch_size, emb_dim)\n\u001b[0;32m      9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,emb_dim))\n",
      "File \u001b[1;32mc:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\einops\\einops.py:483\u001b[0m, in \u001b[0;36mrearrange\u001b[1;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRearrange can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be applied to an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    482\u001b[0m     tensor \u001b[39m=\u001b[39m get_backend(tensor[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[1;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(cast(Tensor, tensor), pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[1;32mc:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\einops\\einops.py:420\u001b[0m, in \u001b[0;36mreduce\u001b[1;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[0;32m    418\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Input is list. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    419\u001b[0m message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAdditional info: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(axes_lengths)\n\u001b[1;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m EinopsError(message \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(e))\n",
      "\u001b[1;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b c (num_w p1)(num_h p2) -> b (num_w num_h)(p1 p2 c)\".\n Input tensor shape: torch.Size([1, 1, 1, 1]). Additional info: {'p1': 16, 'p2': 16}.\n Shape mismatch, can't divide axis of length 1 in chunks of 16"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = vision_transformer()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, (8,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
