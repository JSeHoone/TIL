{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 따라하기 (Quick Start Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader ## Dataset을 순회 가능한 객체로 감싼다. \n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 공개 데이터셋에서 Train 데이터를 받습니다.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "## 공개 데이터셋에서 Test 데이터를 받습니다.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[N C H W] shape의 X : torch.Size([64, 1, 28, 28])\n",
      "shape of y :torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Data Loader를 생성합니다.\n",
    "train_dataloder = DataLoader(training_data,batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data,batch_size = batch_size)\n",
    "\n",
    "for X,y in test_dataloader:\n",
    "    print(f\"[N C H W] shape의 X : {X.shape}\")\n",
    "    print(f\"shape of y :{y.shape}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# model을 정의합니다.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 매개변수 최적화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        # predict error calculate\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, currect = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{currect:>5d} / {size:>5d}]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    num_batchs = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        test_loss /= num_batchs\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " =====================================\n",
      "loss: 0.626866 [    0 / 60000]\n",
      "loss: 0.731875 [ 6400 / 60000]\n",
      "loss: 0.496069 [12800 / 60000]\n",
      "loss: 0.740531 [19200 / 60000]\n",
      "loss: 0.652936 [25600 / 60000]\n",
      "loss: 0.633558 [32000 / 60000]\n",
      "loss: 0.711945 [38400 / 60000]\n",
      "loss: 0.717071 [44800 / 60000]\n",
      "loss: 0.703355 [51200 / 60000]\n",
      "loss: 0.662936 [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.662731 \n",
      "\n",
      "Epoch 2\n",
      " =====================================\n",
      "loss: 0.607315 [    0 / 60000]\n",
      "loss: 0.713495 [ 6400 / 60000]\n",
      "loss: 0.480387 [12800 / 60000]\n",
      "loss: 0.727765 [19200 / 60000]\n",
      "loss: 0.641872 [25600 / 60000]\n",
      "loss: 0.623037 [32000 / 60000]\n",
      "loss: 0.695366 [38400 / 60000]\n",
      "loss: 0.707620 [44800 / 60000]\n",
      "loss: 0.693163 [51200 / 60000]\n",
      "loss: 0.650237 [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.649612 \n",
      "\n",
      "Epoch 3\n",
      " =====================================\n",
      "loss: 0.589541 [    0 / 60000]\n",
      "loss: 0.696552 [ 6400 / 60000]\n",
      "loss: 0.466161 [12800 / 60000]\n",
      "loss: 0.715963 [19200 / 60000]\n",
      "loss: 0.631752 [25600 / 60000]\n",
      "loss: 0.613698 [32000 / 60000]\n",
      "loss: 0.679892 [38400 / 60000]\n",
      "loss: 0.699281 [44800 / 60000]\n",
      "loss: 0.684252 [51200 / 60000]\n",
      "loss: 0.638530 [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.637510 \n",
      "\n",
      "Epoch 4\n",
      " =====================================\n",
      "loss: 0.573323 [    0 / 60000]\n",
      "loss: 0.680936 [ 6400 / 60000]\n",
      "loss: 0.453299 [12800 / 60000]\n",
      "loss: 0.704972 [19200 / 60000]\n",
      "loss: 0.622655 [25600 / 60000]\n",
      "loss: 0.605428 [32000 / 60000]\n",
      "loss: 0.665602 [38400 / 60000]\n",
      "loss: 0.691916 [44800 / 60000]\n",
      "loss: 0.676474 [51200 / 60000]\n",
      "loss: 0.627620 [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.626358 \n",
      "\n",
      "Epoch 5\n",
      " =====================================\n",
      "loss: 0.558433 [    0 / 60000]\n",
      "loss: 0.666637 [ 6400 / 60000]\n",
      "loss: 0.441675 [12800 / 60000]\n",
      "loss: 0.694673 [19200 / 60000]\n",
      "loss: 0.614295 [25600 / 60000]\n",
      "loss: 0.597976 [32000 / 60000]\n",
      "loss: 0.652314 [38400 / 60000]\n",
      "loss: 0.685547 [44800 / 60000]\n",
      "loss: 0.669769 [51200 / 60000]\n",
      "loss: 0.617419 [57600 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.616085 \n",
      "\n",
      "Train Result: Epoch4 Accuracy:0.7872 Avg loss: 0.6160853129283638\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "for t in range(epoch):\n",
    "\n",
    "    print(f\"Epoch {t +1}\\n =====================================\")\n",
    "    train(train_dataloder,model, loss_fn,optimizer)\n",
    "    acc, loss = test(test_dataloader, model,loss_fn)\n",
    "print(f\"Train Result: Epoch{t} Accuracy:{acc} Avg loss: {loss}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
